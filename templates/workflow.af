%embbed_queries){
	. ~soft_bio_267/initializes/init_python
	export PATH=$code_path:$PATH
	export MKL_NUM_THREADS=1 #Pytorch option for CPU
	export OMP_NUM_THREADS=1 #Pytorch option for CPU
	?
	embedding_query.py $model_name $queries $current_model out
}

%get_abstracts){
	resources: -n cal -c 50 -t '7-00:00:00' -m '200gb'
	. ~soft_bio_267/initializes/init_python
	export PATH=$code_path:$PATH
	COUNTER=1
	mkdir indexes
	for f in $pubmed_path/*
	do
		idx_name=`basename -s '.xml.gz' $f`
		?
		get_pubmed_index.py $f indexes/$idx_name.idx& #Un binario a parte (parte de preprocesado)
        if [ $((COUNTER % [cpu])) == 0 ]
        then
        	wait
        fi		
		COUNTER=$[$COUNTER +1]
	done
	wait
}

%embbed_abstracts){
	resources: -n cal -c 36 -t '7-00:00:00' -m '80gb'
	# Rebuild script to load once the model, load each corpus and apply all cpu (use autoflow cpu option in THREADS variables) in one embedding, write it to disk and take the next one
	. ~soft_bio_267/initializes/init_python
	export PATH=$code_path:$PATH
	export MKL_NUM_THREADS=$(([cpu] - 6)) #Pytorch option for CPU
	export OMP_NUM_THREADS=$(([cpu] - 6)) #Pytorch option for CPU
	#WORKERS=$(([cpu] - 5))
	#COUNTER=1
	mkdir embeddings
	for f in get_abstracts)/indexes/*
	do
		idx_name=`basename -s '.idx' $f`
		?
		embedding_corpus.py $model_name $current_model $f embeddings/$idx_name#&
        #if [ $((COUNTER % WORKERS)) == 0 ]
        #then
        #	wait
        #fi		
		#COUNTER=$[$COUNTER +1]
	done		
	#wait
}


query_abstracts){
	. ~soft_bio_267/initializes/init_python
	export PATH=$code_path:$PATH
	export MKL_NUM_THREADS=1 #Pytorch option for CPU
	export OMP_NUM_THREADS=1 #Pytorch option for CPU
	mkdir semantic_scores
	for f in embbed_abstracts)/embeddings/*
	do
		pkl_name=`basename -s '.pkl' $f`
		?
		get_semantic_similarity.py $f embbed_queries)/out.pkl semantic_scores/$pkl_name.txt 20
	done
	awk '{if($3 >= $min_similarity) print $2 "\t" $1 "\t" $3 }' semantic_scores/*.txt > filtered_scores 
	cut -f 1,2 filtered_scores | aggregate_column_data -i - -x 2 -a 1 > pmID_profiles.txt
}


%get_term_profiles){ #Sacarlo para otra plantilla
	resources: -n cal -c 1 -t '7-00:00:00' -m '200gb'
	. ~soft_bio_267/initializes/init_python
	cut -f 1,2 filtered_scores | aggregate_column_data -i - -x 1 -a 2 > pubmed_profiles.txt
	mkdir report
	?
	cohort_analyzer -i pubmed_profiles.txt -o report/pubmed -H -p 1 -d 0 -C 6 -m '' -S ',' #-a
}
