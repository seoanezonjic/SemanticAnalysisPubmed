embbed_queries){
	resources: -n sd -c 30 -t '0-05:00:00'
	. ~soft_bio_267/initializes/init_python

	export PATH=$code_path:$PATH #TODO: Remove later
	source $pyenv/bin/activate #TODO: Remove later

	export MKL_NUM_THREADS=$(([cpu] - 3)) #Pytorch option for CPU
	export OMP_NUM_THREADS=$(([cpu] - 3)) #Pytorch option for CPU
	mkdir out
	?
	stEngine -m $model_name -p $current_model -q $queries -Q out -v
}

get_abstracts){
	resources: -n cal -c 150 -t '1-10:00:00' -m '650gb'
	#resources: -n cal -c 10 -t '7-00:00:00' -m '60gb'	
	. ~soft_bio_267/initializes/init_python

	export PATH=$code_path:$PATH #TODO: Remove later
	source $pyenv/bin/activate #TODO: Remove later
	
	n_cpus=$(([cpu] - 3))
	mkdir indexes
	?
	get_pubmed_index -i "$pubmed_path/*" -o indexes -t "pubmed_" -k $pubmed_chunksize -c $n_cpus -d $splitted $paper $equivalences

	#NEW STEP: CHECKING IF A PREFILTER PMIDs FILE IS GIVEN AND THEN SUBSET CORPUS TO ONLY CONTAIN THOSE PAPERS/ABSTRACTS
	# AND SPLITTING IN DIFFERENT FILES TO END WITH FILE SIZE EQUAL TO PUBMED_CHUNKSIZE
	pmids_prefilter_filepath="$prefilter_pmids_file"
	if [[ -n "$pmids_prefilter_filepath" ]]; then #Check if prefilter_pmids_file variable is not an empty string
		if [[ -s "$pmids_prefilter_filepath" ]]; then 
			zgrep --no-filename -wf $prefilter_pmids_file indexes/*.gz > filtered_corpus_raw
			intersect_columns -a filtered_corpus_raw -b $prefilter_pmids_file -A 1 -B 1 --k c --full | cut -f 1-7 > filtered_corpus
			if [ ! -s filtered_corpus ]; then exit 1; fi #exit if no results 
			rm indexes/*
			n_docs=`cat filtered_corpus | wc -l`
			for start_idx in $(seq 0 `echo "$n_docs/$postfilter_chunksize" | bc`); do
				start_line=`echo "$start_idx*$postfilter_chunksize+1" | bc`
				tail -n +$start_line filtered_corpus | head -n $postfilter_chunksize | gzip > indexes/pubmed_$start_idx.gz
			done
			if [ `ls indexes | wc -l` -eq 0 ]; then exit 1; fi #exit if no results
			#rm filtered_corpus
		fi
	fi
	#END NEW STEP

	#BEGIN:EXPERIMENTAL: TRYING TO PARALLELIZE "N" PUMBED FILLED BALANCED FOLDERS TO DRISTIBUTE THE EMBEDDING PROCESS IN DIFFERENT EXA NODES
	ls indexes > indexes_list
	for fold in `echo $tsv_folder | tr '-' ' '`; do
        mkdir $fold
	done

	total_lines=`wc -l < indexes_list`
	for i in `seq 1 $total_lines`; do
		modulo=$((i % $n_parallel_folders))
		filepath=`sed "$i"'q;d' indexes_list`
		ln -s $PWD/"indexes/"$filepath $PWD/$parallel_folders_basename$modulo/$filepath	
	done
	if [[ ! -d $parallel_folders_basename'0' || `ls $parallel_folders_basename'0' | wc -l` -eq 0 ]]; then exit 1 ; fi
	#END:EXPERIMENTAL

	cat *.err | grep stats | grep UserWarning > abstracts_debug_stats.txt
	cp abstracts_debug_stats.txt $tmp_path/abstracts_debug_stats.txt
}

embbed_abstracts_[$folders_to_parallelize]){
	resources: -n dgx -c 10 -t '1-12:00:00' -m '360gb' -A exclude=exa01;$n_gpus
	#resources: -n dgx -c 10 -t '0-12:00:00' -m '120gb' -A exclude=exa04;gres=gpu:8
	#resources: -n sd -c 46 -t '4-00:00:00' -m '80gb'
	# Rebuild script to load once the model, load each corpus and apply all cpu (use autoflow cpu option in THREADS variables) in one embedding, write it to disk and take the next one
	#. ~soft_bio_267/initializes/init_python
	source ~soft_bio_267/initializes/init_pytorch	

	echo [cpu]
	export PATH=$code_path:$PATH #TODO: Remove later
	source $pyenv/bin/activate #TODO: Remove later

	export MKL_NUM_THREADS=$(([cpu] - 3)) #Pytorch option for CPU
	export OMP_NUM_THREADS=$(([cpu] - 3)) #Pytorch option for CPU
	mkdir embeddings

	gpu_csv=`echo $gpu_devices | tr '-' ','`
	echo "using gpu devices: $gpu_csv"
	?
	stEngine -m $model_name -p $current_model -c get_abstracts)/(*)/"*.gz" -C embeddings -g $gpu_csv -v $splitted
	if [ `ls embeddings | wc -l` -eq 0 ]; then exit 1; fi
}

query_abstracts_[$folders_to_parallelize]){
	#resources: -n cal -c 13 -m 150G
	resources: -n cal -c 150 -m 650G
	. ~soft_bio_267/initializes/init_python

	export PATH=$code_path:$PATH #TODO: Remove later
	source $pyenv/bin/activate #TODO: Remove later

	export MKL_NUM_THREADS=$(([cpu] - 3)) #Pytorch option for CPU
	export OMP_NUM_THREADS=$(([cpu] - 3)) #Pytorch option for CPU
	mkdir semantic_scores
	query_basename=`basename $queries`
	?
	stEngine -C !embbed_abstracts_*!"/embeddings/*" -Q embbed_queries)"/out/"$query_basename".pkl" -k $top_k -o semantic_scores -v -t $soft_min_similarity
	if [[ ! -s semantic_scores/$query_basename ]]; then exit 1; fi #exit if no results
	awk '{print $2 "\t" $1 "\t" $3 }' semantic_scores/$query_basename > soft_filtered_scores_raw
	rm semantic_scores/$query_basename
}

aggregate_results){
	resources: -n cal -c 4 -m '500gb' -t '0-12:00:00'
	. ~soft_bio_267/initializes/init_python
	export PATH=$code_path:$PATH #TODO: Remove later
	source $pyenv/bin/activate #TODO: Remove later	
	query_basename=`basename $queries`
	?
	cat !query_abstracts_!/soft_filtered_scores_raw | awk '{if($3 >= '$hard_min_similarity') print $0}' > hard_filtered_scores_raw
	collapse_same_HPs_inside_of_splitted_abstract.py -i hard_filtered_scores_raw -o hard_filtered_scores
	aggregate_column_data -i hard_filtered_scores -x 2 -a 1,3,4 > llm_term_profiles.txt
	aggregate_column_data -i hard_filtered_scores -x 1 -a 2,3,4 > llm_pmID_profiles_with_cosine_sim.txt

	cut -f 1,2 hard_filtered_scores | semtools -i - -O HPO -c -T "HP:0000118" --2cols --out2cols -o llm_pmID_profiles_cleaned_2cols.txt
	aggregate_column_data -i llm_pmID_profiles_cleaned_2cols.txt -x 1 -a 2 > llm_pmID_profiles_cleaned.txt
	if [ ! -s llm_pmID_profiles_cleaned.txt ]; then exit 1; fi #exit if no results

	if [ -s pubmed_metadata_full ]; then rm pubmed_metadata_full; fi
	for filename in get_abstracts)/indexes/* ; do
		zcat $filename | cut -f 1,3,4,5,6,7 >> pubmed_metadata_full
	done
	
	cp hard_filtered_scores $results_path/llm_filtered_scores
	cp llm_pmID_profiles_with_cosine_sim.txt $results_path/llm_pmID_profiles_with_cosine_sim.txt
	cp llm_pmID_profiles_cleaned.txt $results_path/llm_pmID_profiles.txt
	cp llm_term_profiles.txt $results_path/llm_term_profiles.txt
	cp pubmed_metadata_full $results_path/pubmed_metadata
}
